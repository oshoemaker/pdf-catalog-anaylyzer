{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "## Catalog Product Extraction\n",
        "This notebook guides you through extracting product information from a PDF catalog, even if you are new to the process. You will choose a PDF, set the exact prompt text in one place, and then run a memory‑safe extraction that goes page by page. The system reads text when available and uses the page image when that works better, so it can handle different page layouts without you changing code.\n",
        "\n",
        "Each step has a short description and ends with a single function call. When you run everything, you will get a JSON file (and a CSV for convenience) that lists the products the system found, including fields such as manufacturer, model, years, and part numbers. If results are not what you expect, simply adjust the prompt in the prompt cell and run the steps again; you should not need to edit the library code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Environment and Imports\n",
        "\n",
        "Purpose: Load environment variables and import the refactored library entry points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Auto-reload library modules so edits take effect without kernel restart\n",
        "try:\n",
        "    get_ipython().run_line_magic(\"load_ext\", \"autoreload\")  # type: ignore[name-defined]\n",
        "    get_ipython().run_line_magic(\"autoreload\", \"2\")  # type: ignore[name-defined]\n",
        "except Exception:\n",
        "    pass\n",
        "\n",
        "# Ensure project root on sys.path so we can import the local `lib` package\n",
        "PROJECT_ROOT = Path(\"..\").resolve()\n",
        "if str(PROJECT_ROOT) not in sys.path:\n",
        "    sys.path.insert(0, str(PROJECT_ROOT))\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "from lib import (\n",
        "    PDFAnalyzer,\n",
        "    AIDataExtractor,\n",
        "    process_document,\n",
        "    process_document_streaming,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Configure Inputs\n",
        "\n",
        "Purpose: Choose the PDF file and output locations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PosixPath('/Users/owen/Repos/pdf-catalog-anaylyzer/pdfs/2019_Undercar_Catalog_Small.pdf')"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "PDF_DIR = Path(\"../pdfs\").resolve()\n",
        "EXTRACTED_DIR = Path(\"../extracted_data\").resolve()\n",
        "TEMP_DIR = Path(\"../temp\").resolve()\n",
        "OUTPUT_DIR = Path(\"../output\").resolve()\n",
        "\n",
        "PDF_DIR.mkdir(exist_ok=True, parents=True)\n",
        "EXTRACTED_DIR.mkdir(exist_ok=True, parents=True)\n",
        "TEMP_DIR.mkdir(exist_ok=True, parents=True)\n",
        "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Choose the first PDF by default\n",
        "pdf_files = sorted(PDF_DIR.glob(\"*.pdf\"))\n",
        "selected_pdf = pdf_files[0] if pdf_files else None\n",
        "selected_pdf\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Edit Prompt(s)\n",
        "\n",
        "Purpose: Store prompt text in one cell for easy iteration during analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Edit the exact extraction prompt text here. This is the active prompt.\n",
        "EXTRACTION_PROMPT = \"\"\"\n",
        "You are an expert automotive parts specialist analyzing a catalog page. Use your deep understanding of automotive systems to extract contextually accurate product information.\n",
        "\n",
        "For each product/fitment entry, extract a JSON array of objects with:\n",
        "- manufacturer, model, years, specifications, side, inner_part_number, outer_part_number, part_number, category, notes\n",
        "\n",
        "Rules:\n",
        "- Preserve exact part numbers and year formats\n",
        "- If unsure about a field, use null\n",
        "- Return only a JSON array; no extra commentary\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Analyze PDF Structure\n",
        "\n",
        "Purpose: Load the PDF, extract text for all pages, and create per-page images for vision tasks (memory-safe).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting text: 100%|██████████| 20/20 [00:01<00:00, 12.60it/s]\n",
            "Processing pages: 100%|██████████| 20/20 [00:01<00:00, 12.34it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 2019_Undercar_Catalog_Small.pdf with 20 pages\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "assert selected_pdf is not None, \"Add a PDF to pdfs/ before proceeding.\"\n",
        "\n",
        "# Streaming-friendly setup: load PDF and basic info only\n",
        "analyzer = PDFAnalyzer(selected_pdf, temp_dir=TEMP_DIR, extracted_data_dir=EXTRACTED_DIR)\n",
        "loaded = analyzer.load_pdf()\n",
        "\n",
        "if loaded:\n",
        "    pdf_info = analyzer.extract_basic_info()\n",
        "    print(f\"Loaded {pdf_info['filename']} with {pdf_info['page_count']} pages\")\n",
        "else:\n",
        "    raise RuntimeError(\"Failed to load PDF\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Page Strategy and Product Page Detection\n",
        "\n",
        "Purpose: Identify likely product pages to prioritize hybrid extraction.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Identified 18 likely product pages out of 20\n"
          ]
        }
      ],
      "source": [
        "# Page classification to focus extraction on product pages\n",
        "product_pages_analysis = analyzer.detect_product_data_pages()\n",
        "num_product_pages = sum(1 for p in product_pages_analysis if p.get(\"is_product_page\"))\n",
        "print(f\"Identified {num_product_pages} likely product pages out of {len(product_pages_analysis)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6 (Option A): Streaming Hybrid Extraction\n",
        "\n",
        "Purpose: Process pages one-by-one with a single reusable AdvancedDocumentAI instance to minimize memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Streaming 20 pages in low-memory mode using OpenAI...\n",
            "Page 1/20: candidate=False text_len=0\n",
            "  Extracted 0 items this page | Total so far: 0\n",
            "Page 2/20: candidate=True text_len=2437\n",
            "  Rendered image for page 2 -> page_2.png\n",
            "  Extracted 0 items this page | Total so far: 0\n",
            "Page 3/20: candidate=False text_len=4568\n",
            "  Extracted 0 items this page | Total so far: 0\n",
            "Page 4/20: candidate=True text_len=6783\n",
            "  Rendered image for page 4 -> page_4.png\n",
            "  Extracted 0 items this page | Total so far: 0\n",
            "Page 5/20: candidate=True text_len=1766\n",
            "  Rendered image for page 5 -> page_5.png\n",
            "  Extracted 3 items this page | Total so far: 3\n",
            "Page 6/20: candidate=True text_len=2104\n",
            "  Rendered image for page 6 -> page_6.png\n",
            "  Extracted 12 items this page | Total so far: 15\n",
            "Page 7/20: candidate=True text_len=3004\n",
            "  Rendered image for page 7 -> page_7.png\n",
            "  Extracted 77 items this page | Total so far: 92\n",
            "Page 8/20: candidate=True text_len=3201\n",
            "  Rendered image for page 8 -> page_8.png\n",
            "  Extracted 49 items this page | Total so far: 141\n",
            "Page 9/20: candidate=True text_len=3190\n",
            "  Rendered image for page 9 -> page_9.png\n",
            "  Extracted 82 items this page | Total so far: 223\n",
            "Page 10/20: candidate=True text_len=6185\n",
            "  Rendered image for page 10 -> page_10.png\n",
            "  Extracted 50 items this page | Total so far: 273\n",
            "Page 11/20: candidate=True text_len=6225\n",
            "  Rendered image for page 11 -> page_11.png\n",
            "  Extracted 47 items this page | Total so far: 320\n",
            "Page 12/20: candidate=True text_len=6944\n",
            "  Rendered image for page 12 -> page_12.png\n",
            "  Extracted 54 items this page | Total so far: 374\n",
            "Page 13/20: candidate=True text_len=6979\n",
            "  Rendered image for page 13 -> page_13.png\n",
            "  Extracted 54 items this page | Total so far: 428\n",
            "Page 14/20: candidate=True text_len=6857\n",
            "  Rendered image for page 14 -> page_14.png\n",
            "  Extracted 52 items this page | Total so far: 480\n",
            "Page 15/20: candidate=True text_len=5929\n",
            "  Rendered image for page 15 -> page_15.png\n",
            "  Extracted 50 items this page | Total so far: 530\n",
            "Page 16/20: candidate=True text_len=6499\n",
            "  Rendered image for page 16 -> page_16.png\n",
            "  Extracted 52 items this page | Total so far: 582\n",
            "Page 17/20: candidate=True text_len=6065\n",
            "  Rendered image for page 17 -> page_17.png\n",
            "  Extracted 56 items this page | Total so far: 638\n",
            "Page 18/20: candidate=True text_len=6478\n",
            "  Rendered image for page 18 -> page_18.png\n",
            "  Extracted 54 items this page | Total so far: 692\n",
            "Page 19/20: candidate=True text_len=6239\n",
            "  Rendered image for page 19 -> page_19.png\n"
          ]
        }
      ],
      "source": [
        "# Run streaming hybrid extraction (lower memory)\n",
        "stream_result = process_document_streaming(\n",
        "    pdf_path=selected_pdf,\n",
        "    temp_dir=TEMP_DIR,\n",
        "    extracted_data_dir=EXTRACTED_DIR,\n",
        "    extraction_prompt=EXTRACTION_PROMPT,\n",
        "    catalog_type=\"automotive\",\n",
        "    use_openai=True,\n",
        "    use_gemini=False,\n",
        "    use_advanced_ai=True,\n",
        "    high_quality_images=False,\n",
        "    minimize_memory=True,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "extracted_items = stream_result.get(\"items\", [])\n",
        "len(extracted_items)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Save Outputs\n",
        "\n",
        "Purpose: Persist extracted data to JSON/CSV for downstream analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "if 'selected_pdf' not in globals() or selected_pdf is None:\n",
        "    raise RuntimeError(\"No PDF selected. Run Step 2-4 first.\")\n",
        "if 'EXTRACTED_DIR' not in globals() or 'OUTPUT_DIR' not in globals():\n",
        "    raise RuntimeError(\"Output directories not configured. Run Step 2 first.\")\n",
        "if 'extracted_items' not in globals():\n",
        "    print(\"No extracted_items found. Run Step 6 first.\")\n",
        "    extracted_items = []\n",
        "json_path = EXTRACTED_DIR / f\"{selected_pdf.stem}_items.json\"\n",
        "csv_path = OUTPUT_DIR / f\"{selected_pdf.stem}_items.csv\"\n",
        "with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(extracted_items, f, ensure_ascii=False, indent=2)\n",
        "if extracted_items:\n",
        "    pd.DataFrame(extracted_items).to_csv(csv_path, index=False)\n",
        "    result_paths = (json_path, csv_path)\n",
        "else:\n",
        "    result_paths = (json_path, None)\n",
        "result_paths\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
